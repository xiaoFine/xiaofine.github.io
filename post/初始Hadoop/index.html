<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>初始Hadoop | xiaoFine</title>
<meta name="description" content="有六层楼那么高">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://xiaofine.github.io/favicon.ico?v=1560673184600">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://xiaofine.github.io/styles/main.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  


<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>

<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-142181486-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-142181486-1');
</script>


  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://xiaofine.github.io">
        <img src="https://xiaofine.github.io/images/avatar.png?v=1560673184600" class="site-logo">
        <h1 class="site-title">xiaoFine</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
          <a class="social-link" href="https://xiaofine.github.io/" target="_blank">
            <i class="fab fa-github"></i>
          </a>
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      有六层楼那么高
    </div>
    <div class="site-footer">
       | <a class="rss" href="https://xiaofine.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">初始Hadoop</h2>
            <div class="post-date">2018-03-16</div>
            
            <div class="post-content">
              <h3 id="hadoop简介">Hadoop简介</h3>
<p>Hadoop是一个于2011年，由Apache基金会所开发的分布式系统基础架构。它为我们提供了一个可靠的，可扩展的分布式计算框架。总的来说，Hadoop由四部分组成，包括:</p>
<ol>
<li>分布式计算框架MapReduce</li>
<li>分布式文件系统HDFS</li>
<li>作业调度和资源管理服务YARN</li>
<li>用于支持外部组件的公用调用服务Hadoop Common</li>
</ol>
<p>本文主要就前三部分进行总结。</p>
<!--more-->
<p>就分布式计算框架而言可以分为 <strong>模型并行(Model Parallelism)</strong> ， <strong>数据并行(Data Parallelism)</strong> 和 <strong>两者的混合(Irregular Parallelism)</strong> 。模型并行化是将模型划分成子模型，分布到各个节点上（比如将线性模型参数W根据feature进行划分）；数据并行是将数据划分到不同节点上，而计算模型本身不会进行划分。
{% asset_img 20170729203732772.png parallelismModel1%}
{% asset_img 20170729203751273.png parallelismModel2%}
根据算法迭代方法的不同，现有的分布式计算框架可以划分为 <strong>基于同步(Synchronization-based)</strong> 的和 <strong>基于异步的(Asynchronization-based)</strong>。两者的不同在于是否等待所有节点完成计算/任务，然后在进入下一次迭代/任务。</p>
<p>对于Hadoop框架来说，Hadoop是一个 <strong>基于同步和数据并行(Synchronization-based and Data-distributed)的计算框架</strong>。Hadoop是建立在HDFS之上，数据本身已经进行了分布式存储与各个节点上，所以Hadoop的MapReduce计算任务会将整个计算模型/程序发送到各个节点上运行。这暗示着Hadoop不可避免的会涉及到数据在不同节点上的调度，同时带来巨大的IO开销。并且也暗示了MapReduce计算框架不能应用于数据之间有依赖的情况。因为HDFS上的数据是基于IID划分的。另一方面，对于每一次MapReduce任务迭代，MapReduce总是等到所有节点都完成了Map操作之后，再进行Reduce操作。当某个节点执行任务特别慢的时候，整个集群会陷入等待状态。考虑到这个问题Hadoop也做了相应的处理。比如当一定比例的节点完成Map任务的时候，就可以进入Reduce阶段，而不在等待；当然也在从任务调度的层面监控作业进度，将某个节点上执行缓慢的任务放在别的节点上执行。</p>
<p>即便Hadoop本身有一些不足，但是也不影响Hadoop的优势：</p>
<ul>
<li>Low cost，搭建Hadoop的成本是很小的，它能利用每一个臭皮匠</li>
<li>Scalability，多个臭皮匠，顶很多诸葛亮</li>
<li>Reliability and Fault Tolerance，Hadoop本身提供很多的容错机制，保证集群的可靠性。包括HDFS层面的冗余备份，HA机制对节点宕机的容错，YARN机制对作业调度的容错支持等等。</li>
<li>Efficiency，分布式计算的优势</li>
<li>Throughput，支持很大的吞吐量，支持一写多读</li>
</ul>
<p>当然，Hadoop也有不能处理的场景</p>
<ol>
<li>小数据与大量小文件。由于HDFS上数据是以块(Block)进行存储的，不足块大小的文件同样会占用一个块的大小。如果小文件太多，会占用NameNode过多的内存来维护元数据信息。</li>
<li>修改文件的值。HDFS不提供Value-level的数据修改，只能覆写或者追加</li>
<li>流式作业和低延时场景。Hadoop MapReduce是操作HDFS上的数据，所以流式数据到来之后，会涉及到在HDFS上的转存操作，然后再是读取数据，交于MapReduce计算。并且MapReduce计算框架本身也会涉及到一些数据溢写的IO操作。因此实时性要求强的场景下，Hadoop并不具有优势</li>
<li>MapReduce多次迭代。Hadoop对于MapReduce的迭代任务的支持并不友好，需要手动构建多个MapReduce Job。对于众多机器学习算法，算法的迭代是难以避免的。这时候Hadoop不是一个很好的选择</li>
<li>基于图的并行计算（Graph parallel computing）。Hadoop是基于数据并行的计算框架，底层HDFS进行数据分布式储存的时候，并没有考虑数据之间的联系，反倒是假设了数据是IID的。因此对于图数据，Hadoop也是不适合的。</li>
</ol>
<p>####Hadoop架构
HDFS是Hadoop Distributed File System的缩写，它采用了主从式的结构。根据节点角色的不同，可以主要分为 <strong>NameNode</strong> 和 <strong>DataNode</strong> （细分还包括Second Namenode，JournalNode等等）。下图是HDFS的粗略框架
{% asset_img 微信图片_20180725142528.jpg HadoopConstruction%}</p>
<p>其中</p>
<ul>
<li><strong>Namenode</strong> 充当班长的角色，管理着整个班级（集群）的信息。这份名单被叫做元数据（meta-data）。另外，客户端的访问请求都由namenode负责。另外namenode上也维护了集群的其他信息，包括命名空间，集群配置信息等等。一般来说，namenode上有两个重要文件常驻内存，提供元数据信息。这两个文件是FsImage和EditLogs。前者记录了命名空间，集群配置信息和数据块的相关信息；后者存放了对文件系统的每一次修改记录。比如新建了一个文件夹，修改了数据块的复制个数等等。Namenode需要周期性的合并两个文件，生成新的FsImage文件，以提供客户端的查询访问。</li>
<li><strong>Datanode</strong> 负责实际数据的存放，数据以数据块（block）的形式存储在datanode之上，每一个数据块都有多个冗余备份（Replication），以达到容错的作用。客户端对数据的读写操作直接作用在datanode上，但前提是客户端首先得由namenode告知在哪里读，在哪里写。</li>
<li>图中的Rack是表示一个PC或者机柜</li>
</ul>
<p>为了保证整个班级的正常次序，班长需要定期的点名，检查是否有童鞋上课睡觉。这里的namenode并不会主动点名，而是让各个datanode主动汇报，定期的给namenode发送信息。信息包括两部分，<strong>心跳检测（HeartBeat）</strong> 和 <strong>数据块汇报信息（BlockReport）</strong>。前者告诉namenode，datanode还活着；后者是该datanode上数据块的信息。</p>
<p>如果HeartBeat和BlockReport被namenode收到了
namenode会汇总大家的blockreport，检测文件的数据块是否有丢失，数据块的复制数是否达到要求。如果没有达到要求，集群会进入 <strong>Safe Mode</strong>。如果namenode超过10min还没有收到某个datanode的HeartBeat
namenode会将这个datanode标记为挂了，然后将原本存储在这个datanode上的数据块，重新复制到其他节点上，并且以后的计算任务也不会再发送给这个datanode了。
当然，班长namenode也可能会挂掉。一旦班长挂了，整个班级都约等于挂了。为了防止这个情况，允许集群的更好的容错能力，Hadoop 1.X启动了副班长策略，引入了 <strong>Second Namenode</strong>。Second namenode负责FsImage和EditLog两个文件的合并，减轻namenode的负担（如下图）。
{%asset_img 微信图片_20180725144501.jpg SecondNN%}</p>
<p>当namenode挂掉了，second namenode上会保存有最新的FsImage文件。那么集群管理员就可以将这份FsImage拷贝到namenode上，然后人工重启namenode。所以second namenode不提供namenode的故障处理，它仅仅是减轻namenode的压力而已。</p>
<p>要达到故障处理的要求，Hadoop 2.X之后提供了 <strong>Hadoop HA策略</strong>。但是，注意，<strong>HA和second namenode策略不能同时使用</strong>。Hadoop HA的框架图如下：
{%asset_img 微信图片_20180725144658.jpg HA%}</p>
<p>Hadoop HA引入了两个Namenode，即 <strong>Active Namename</strong> 和 <strong>Standby Namenode</strong>。但是只有一个提供集群服务，而另一个就standby。一旦active namenode挂了，standby的就立刻上线。至于一开始，哪个namenode充当active的，取决于Zookeeper。Zookeeper提供master选举的作用，这个选举实际上是一个抢占式的锁机制，两个namenode谁先到谁就是active的。</p>
<p>standby namenode需要和active namenode的元信息一致，才能在active one宕掉之后，立刻提供一致服务。为了让元信息一致：</p>
<ol>
<li>集群里的datanode需要同时向两个namenode发送心跳和blockreport；</li>
<li>Active namenode对EditLog的修改需要同时写入JournalNodes（也就是图中的JN）。一般来说，有2N+1台JN来存储EditLog，每次写数据操作有大多数（&gt;=N+1）返回成功时即认为该次写成功。同时，也容忍了最多有N台机器挂掉，如果多于N台挂掉，这个算法就失效了。</li>
<li>任何修改操作在 Active NN上执行时，JN进程同时也会记录修改log到至少半数以上的JN中，这时 Standby NN 监测到JN 里面的同步log发生变化了会读取 JN 里面的修改log，然后同步到自己的的目录镜像树里面</li>
</ol>
<p>最后，图中的FailoverController监控了NameNode的状态，并且通过心跳机制，监控自己在zookeeper中的状态。一旦active namenode挂了，则触发standby的namenode上线。</p>
<p>Hadoop HA机制虽然提供了故障处理，但是它任然限制了只能有一个namenode提供服务。并且，如果hdfs中有很多块，那么元数据将占用namenode大量的内存。为了处理这个问题，Hadoop 2.X提供了HDFS Federation机制（NN联邦），它允许多个namenode共用一个集群里的存储资源，每个namenode都可以单独对外提供服务。</p>
<p>####Hadoop读写
在HDFS中，数据是以数据块（Block）的形式存储，不同版本Hadoop的默认块大小不同，128M或者64M。这个值可以用户自己定义。如果一个数据文件被划分成越小的数据块，HDFS读取这个文件时候的并发性也就更高。但是，也意味着将带来更多的磁盘寻道的IO开销，所以这是个trade-off。
HDFS采用了冗余备份的策略，为每一个数据块都保存了多个复制（replicas）。默认replicas的大小为3份。下面展示了HDFS上数据的存储图：
{% asset_img 微信图片_20180725145432.jpg Split%}</p>
<ul>
<li>图中可以再次看到namenode上存储了文件到数据块的映射信息，比如文件part-0，有两个数据块1,3。每个数据块一共有两个复制，分别存储在不同的节点上。事实上，数据块的总数之间影响了MapReduce过程中，mapper的个数。</li>
<li>大文件被划分成了很多的block，而不足block大小的文件（比如一个只有1k的文件），同样占用了一个block。</li>
<li>HDFS对文件的划分存储并没有考虑到文件的结构信息。这时候，HDFS引入了InputSplit，InputSplit由用户在读取HDFS数据的时候指定，它保证了一个文件的逻辑划分。比如，下图是一个由4条记录组成的文件，每条记录100M。
{%asset_img 微信图片_20180725152143.jpg SplitExample1%}
这时候，启动一个mapper来解析任何一个Block都不能都到正确的结果，比较record的结构以及被破坏。为了处理这里问题，我们需要指定一个InputSplit来读取数据，给出每一条record在逻辑上的划分。PS：普通文本文件当然可以不用设置InputSplit。
{%asset_img 微信图片_20180725152247.jpg SplitExmaple2%}
具体来说，需要在设置Job的时候，调用job.setInputFormatClass(WholeFileInputFormat.class)来指定自己实现的InputSplit格式。这里的WholeFileInputFormat.class是自己实现的类，用来指定split，它需要继承FileInputFormat类（当然选择TextInputFormat，SequenceFileInputFormat等等类来集成也是可以的，具体场景，具体分析），覆写createRecordReader和isSplitable方法。同时还要指定自己的RecordReader。下面是一个fengfeng写的小栗子，用于解析一个二进制文件。</li>
</ul>
<pre><code>public class WholeFileInputFormat extends FileInputFormat&lt;Text, BytesWritable&gt; {
    @Override
    public RecordReader&lt;Text, BytesWritable&gt; createRecordReader(
            InputSplit split, TaskAttemptContext context) throws IOException,
            InterruptedException {
      //这个类告诉程序要怎么去读文件，找出正确的input划分
        WholeFileRecordReader recordReader = new WholeFileRecordReader();  
        recordReader.initialize(split,context);
        return recordReader;
    }
    @Override
    protected boolean isSplitable(JobContext context, Path filename) {
        return false;
    }
}
 
public class WholeFileRecordReader extends RecordReader&lt;Text, BytesWritable&gt; {
    private FileSplit fileSplit;
    private FSDataInputStream fin;
    private Text key = new Text();
    private BytesWritable value = new BytesWritable();
    private boolean processed = false;
    private Configuration conf;
    private String fileName;
    private int count=0;
    @Override
    public void initialize(InputSplit inputSplit, TaskAttemptContext context)
            throws IOException, InterruptedException {
        //这里整一些初始化的工作
    }
    @Override
    public boolean nextKeyValue() {
        // 这个方法里需要实现具体这么解析一条一条的记录，然后将读取结果设置到key和value里
        // 期间，有任何解析问题就返回false，否则返回true
        value = new BytesWritable(info);
         key.set(count+&quot;&quot;);
         return true;
    }
    
    @Override
    public float getProgress() throws IOException, InterruptedException {
        // TODO Auto-generated method stub
        return processed? fileSplit.getLength():0;
    }
    @Override
    public void close() throws IOException {
        // TODO Auto-generated method stub
        //fis.close();
    }
    @Override
    public Text getCurrentKey() throws IOException, InterruptedException {
        // TODO Auto-generated method stub
        return this.key;
    }
    @Override
    public BytesWritable getCurrentValue() throws IOException,
            InterruptedException {
        // TODO Auto-generated method stub
        return this.value;
    }
}
</code></pre>
<p>在了解了数据是怎么存储了之后，我们再来了解下客户端是如何读写数据的。
#####读数据
由于namenode上存储了datanode和数据块的路径地址，所以，客户端实际读取数据之前，需要访问namenode获取相关信息。而namenode会计算最佳读取的块，然后返回其位置信息给客户端。最后客户端需要根据返回的信息，自己去找datanode读取数据。流程如下图所示。
{%asset_img 微信图片_20180725155109.jpg %}
幸运的是，客户端具体的程序调用，已经屏蔽了读取数据的细节：</p>
<pre><code>Configuration configuration = new Configuration();
String dataPath = &quot;hdfs://localhost:9000/dml&quot;;
FileSystem fileSystem = FileSystem.get(URI.create(dataPath), configuration);
FSDataInputStream fsDataInputStream = fileSystem.open(new Path(dataPath));
BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(fsDataInputStream));
String line = bufferedReader.readLine();
while(line != null){
    System.out.println(line);
    line = bufferedReader.readLine();
}
bufferedReader.close();
fsDataInputStream.close();
fileSystem.close();
</code></pre>
<ol>
<li>客户端(client)用FileSystem的open()函数打开文件</li>
<li>DistributedFileSystem用RPC调用namenode，得到文件的数据块信息。对于每一个数据块，namenode节点返回保存数据块的数据节点的地址。DistributedFileSystem返回FSDataInputStream给客户端，用来读取数据。</li>
<li>客户端调用stream的read()函数开始读取数据。</li>
<li><strong>DFSInputStream</strong> 连接保存此文件第一个数据块的最近的数据节点。Data从数据节点读到客户端(client).
当此数据块读取完毕时，DFSInputStream关闭和此数据节点的连接，然后连接此文件下一个数据块的最近的数据节点。</li>
<li>当客户端读取完毕数据的时候，调用FSDataInputStream的close函数。在读取数据的过程中，如果客户端在与数据节点通信出现错误，则尝试连接包含此数据块的下一个数据节点。失败的数据节点将被记录，以后不再连接。</li>
</ol>
<p>#####写数据
客户端写数据的流程如下所示：
<img src="/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20180725160449.jpg" alt="ReadData">
同样，程序实现：</p>
<pre><code>Configuration configuration = new Configuration();
String dst = &quot;hdfs://localhost:9000/dml&quot;;
String data = &quot;data mining lab data mining lab,data mining lab&quot;;
FileSystem fileSystem = FileSystem.get(URI.create(dst), configuration);
Path path = new Path(dst);
FSDataOutputStream fsDataOutputStream = fileSystem.create(path);
fsDataOutputStream.write(data.getBytes());
fsDataOutputStream.flush();
fsDataOutputStream.close();
</code></pre>
<ol>
<li>客户端调用create()来创建文件</li>
<li>DistributedFileSystem用RPC调用元数据节点，在文件系统的命名空间中创建一个新的文件。元数据节点首先确定文件原来不存在，并且客户端有创建文件的权限，然后创建新文件。DistributedFileSystem返回DFSOutputStream，客户端用于写数据。</li>
</ol>
<blockquote>
<p>RPC(Remote Procedure Call)：一种通信协议，该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用</p>
</blockquote>
<ol start="3">
<li>客户端开始写入数据，DFSOutputStream将数据分成块，写入data queue。</li>
<li>Data queue由Data Streamer读取，并通知元数据节点分配数据节点，用来存储数据块(每块默认复制3块)。分配的数据节点放在一个pipeline里。</li>
<li>Data Streamer将数据块写入pipeline中的第一个数据节点。第一个数据节点将数据块发送给第二个数据节点。第二个数据节点将数据发送给第三个数据节点。DFSOutputStream为发出去的数据块保存了ack queue，等待pipeline中的数据节点告知数据已经写入成功。如果数据节点在写入的过程中失败：关闭pipeline，将ack queue中的数据块放入data queue的开始。当前的数据块在已经写入的数据节点中被元数据节点赋予新的标示，则错误节点重启后能够察觉其数据块是过时的，然后会被删除。失败的数据节点从pipeline中移除，另外的数据块，则写入pipeline中的另外两个数据节点。元数据节点则被通知此数据块是复制块数不足，将来会再创建第三份备份。</li>
<li>当客户端结束写入数据，则调用stream的close函数。</li>
<li>此操作将所有的数据块写入pipeline中的数据节点，并等待ack queue返回成功。最后通知元数据节点写入完毕。</li>
</ol>

            </div>
            
            

            
              
                <div id="gitalk-container" data-aos="fade-in"></div>
              

              
            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>

<script type="application/javascript">

AOS.init();

hljs.initHighlightingOnLoad()

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>



  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: '7b80890ef0a536f3d053',
        clientSecret: 'c6261dccf025200fb3ea1d9e5db9d7be9e073f3a',
        repo: 'xiaofine.github.io',
        owner: 'xiaoFine',
        admin: ['xiaoFine'],
        id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
