<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://xiaofine.github.io</id>
    <title>Gridea</title>
    <updated>2019-06-16T07:59:42.390Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://xiaofine.github.io"/>
    <link rel="self" href="https://xiaofine.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://xiaofine.github.io/images/avatar.png</logo>
    <icon>https://xiaofine.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[Spark Submit中添加多个jar]]></title>
        <id>https://xiaofine.github.io/post/spark-submit-add-jar</id>
        <link href="https://xiaofine.github.io/post/spark-submit-add-jar">
        </link>
        <updated>2019-03-24T05:16:12.000Z</updated>
        <summary type="html"><![CDATA[<p>这次项目需要利用spark读取hbase，但在提交spark任务之后发现该集群没有为spark配置hbase相关的依赖，同时我们没有权限修改集群配置，因此只能通过spark submit时添加<code>--jar</code>参数来指定依赖。</p>
]]></summary>
        <content type="html"><![CDATA[<p>这次项目需要利用spark读取hbase，但在提交spark任务之后发现该集群没有为spark配置hbase相关的依赖，同时我们没有权限修改集群配置，因此只能通过spark submit时添加<code>--jar</code>参数来指定依赖。</p>
<!--more-->
<h4 id="遇到的问题">遇到的问题</h4>
<p>Spark在提交任务时，可以通过--jar参数来添加依赖包，当有个多个jar包时，则以逗号间隔。我们希望<code>--jar</code>参数包含hbase库目录下的所有依赖包，即以<code>--jar /hbase/lib/*.jar</code>的方式进行指定，发现总是会报找不到主类的错误。经过排查发现，<code>lib/*.jar</code>会报目录下的所有jar包以<strong>空格</strong>间隔，导致spark submit命令的参数出现偏倚而无法发现主类。</p>
<h4 id="简单的解决方法">简单的解决方法</h4>
<p>将spark命令修改为</p>
<pre><code>--jar $(echo hbase/lib/*.jar | tr ' ' ',')
</code></pre>
<p><code>tr</code>命令能够将后两个字符集进行替换操作，也因此在处理文件名包含空格的jar包时会出错。</p>
<h4 id="更好的解决方法">更好的解决方法</h4>
<pre><code>--jar $(files=(hbase/lib/*.jar); IFS=,; echo &quot;${files[*]}&quot;)
</code></pre>
<p><code>IFS</code>是域内间隔符(internal field separator)，这里设置为<code>,</code>。</p>
<h5 id="参考资料">参考资料</h5>
<ol>
<li><a href="http://www.learn4master.com/big-data/spark/spark-submit-multiple-jars">spark submit multiple jars</a></li>
<li><a href="https://stackoverflow.com/questions/4729863/using-bash-how-do-you-make-a-classpath-out-of-all-files-in-a-directory/4729899#4729899">Stackoverflow:Using bash, how do you make a classpath out of all files in a directory?</a></li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://xiaofine.github.io/post/hello-gridea</id>
        <link href="https://xiaofine.github.io/post/hello-gridea">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="http://hvenotes.fehey.com/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>𝖶𝗂𝗇𝖽𝗈𝗐𝗌</strong> 或 <strong>𝖬𝖺𝖼𝖮𝖲</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yarn]]></title>
        <id>https://xiaofine.github.io/post/Yarn</id>
        <link href="https://xiaofine.github.io/post/Yarn">
        </link>
        <updated>2018-03-20T14:26:26.000Z</updated>
        <summary type="html"><![CDATA[<h4 id="yarn-mrv1">Yarn MRv1</h4>
<p>Hadoop的MapReduce原有架构主要由以下几个组件组成：Client、JobTracker、TaskTracker、Task。其架构图如下所示：
{%asset_img 20170808210130242.png%}
从上图中可以清楚的看出原 MapReduce 程序的流程及设计思路：</p>
<ol>
<li>首先用户程序 (JobClient) 提交了一个 job，job 的信息会发送到 <strong>Job Tracker</strong> 中，Job Tracker 是 Map-reduce 框架的中心，他需要与集群中的机器定时通信 (heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理所有 job 失败、重启等操作。</li>
<li><strong>TaskTracker</strong> 是 Map-reduce 集群中每台机器都有的一个部分，他做的事情主要是监视自己所在机器的资源情况。</li>
<li>TaskTracker 同时监视当前机器的 tasks 运行状况。TaskTracker 需要把这些信息通过 heartbeat 发送给 JobTracker，JobTracker 会搜集这些信息以给新提交的 job 分配运行在哪些机器上。上图虚线箭头就是表示消息的发送 - 接收的过程。</li>
</ol>
]]></summary>
        <content type="html"><![CDATA[<h4 id="yarn-mrv1">Yarn MRv1</h4>
<p>Hadoop的MapReduce原有架构主要由以下几个组件组成：Client、JobTracker、TaskTracker、Task。其架构图如下所示：
{%asset_img 20170808210130242.png%}
从上图中可以清楚的看出原 MapReduce 程序的流程及设计思路：</p>
<ol>
<li>首先用户程序 (JobClient) 提交了一个 job，job 的信息会发送到 <strong>Job Tracker</strong> 中，Job Tracker 是 Map-reduce 框架的中心，他需要与集群中的机器定时通信 (heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理所有 job 失败、重启等操作。</li>
<li><strong>TaskTracker</strong> 是 Map-reduce 集群中每台机器都有的一个部分，他做的事情主要是监视自己所在机器的资源情况。</li>
<li>TaskTracker 同时监视当前机器的 tasks 运行状况。TaskTracker 需要把这些信息通过 heartbeat 发送给 JobTracker，JobTracker 会搜集这些信息以给新提交的 job 分配运行在哪些机器上。上图虚线箭头就是表示消息的发送 - 接收的过程。</li>
</ol>
<!--more-->
<p>可以看得出原来的 map-reduce 架构是简单明了的，在最初推出的几年，也得到了众多的成功案例，获得业界广泛的支持和肯定，但随着分布式系统集群的规模和其工作负荷的增长，原框架的问题逐渐浮出水面，主要的问题集中如下：</p>
<ol>
<li>JobTracker 是 Map-reduce 的集中处理点，存在 <strong>单点故障</strong>。</li>
<li>JobTracker 完成了太多的任务，造成了 <strong>过多的资源消耗</strong>，当 map-reduce job 非常多的时候，会造成很大的内存开销，潜在来说，也增加了 JobTracker fail 的风险，这也是业界普遍总结出老 Hadoop 的 Map-Reduce 只能支持 4000 节点主机的上限。</li>
<li>在 TaskTracker 端，以 map/reduce task 的数目作为资源的表示过于简单，没有考虑到 cpu/ 内存的占用情况，如果两个大内存消耗的 task 被调度到了一块，很容易出现 OOM。</li>
<li>在 TaskTracker 端，把资源强制划分为 map task slot 和 reduce task slot, 如果当系统中只有 map task 或者只有 reduce task 的时候，会造成资源的浪费，也就是前面提过的集群资源利用的问题。</li>
<li>源代码层面分析的时候，会发现代码非常的难读，常常因为一个 class 做了太多的事情，代码量达 3000 多行，，造成 class 的任务不清晰，增加 bug 修复和版本维护的难度。</li>
<li>从操作的角度来看，现在的 Hadoop MapReduce 框架在有任何重要的或者不重要的变化 ( 例如 bug 修复，性能提升和特性化 ) 时，都会强制进行系统级别的升级更新。更糟的是，它不管用户的喜好，强制让分布式集群系统的每一个用户端同时更新。这些更新会让用户为了验证他们之前的应用程序是不是适用新的 Hadoop 版本而浪费大量时间。</li>
</ol>
<h4 id="yarn">Yarn</h4>
<p>Yarn是Hadoop 2.0以后引入的资源管理系统，它通过资源的动态申请，让Hadoop集群的资源分配变得更加的细致，避免资源浪费（Hadoop 2.0以前map和reducer执行内存是固定的）。Hadoop Yarn一共有三个组成部分：</p>
<ol>
<li>一个ResourceManager：主管整个集群资源的管理和分配；任务调度</li>
<li>各个节点上的NodeManager：主管一个节点的资源情况，比如cpu，内存</li>
<li>对应一个应用的ApplicationMaster：负责监管某个应用（也就是一个job），以及任务的失败重启
下面是Yarn的框架图：
{%asset_img 微信图片_20180725162408.jpg%}
图中有两个client，分别启动了两个applications（看颜色）。一个任务的具体执行者是一个或者多个container，client和container（或者说任务的执行）之间是通过ResourceManager来联系起来的。而ResourceManager通过与Nodemanager的通信，获取每个节点的资源情况，进而掌握整个集群的资源情况。client直接向ResourceManager提交application，而不和具体的worker节点通信。则ResourceManager首先会启动一个applicationmaster，然后该applicationmaster再向resourcemanager申请计算资源，以在更多的节点上开启container进行计算任务。同时，还可以看出，container会向applicationmaster汇报任务的进展情况。</li>
</ol>
<p>具体来说：</p>
<ol>
<li>ResourceManager由两部分组成。用于调度集群里Container的Scheduler 和用于响应client的作业请求，监管applicationmasters的applicationmanager（是的，名字很绕）。在Hadoop中实现的调度器主要有Capacity Scheduler和Fair Scheduler（具体请百度）</li>
<li>container是具体的执行引擎，计算任务就是跑在它里面。一个application会对于一个或多个container，一个节点上也可以运行多个container。不过container也是一个计算资源（cpu，内存）的抽象封装
NodeManager掌管节点的cpu，内存，网络等等资源，同时会把这些信息告诉给ResourceManager，以便做资源统计和任务调度</li>
</ol>
<p>以下面的作业提交流程图为例，看看能不能说得更明白
{%asset_img 微信图片_20180725162914.jpg%}</p>
<ol>
<li>用户通过job.waitForCompletion 方法提交作业</li>
<li>首先会与ResourceManager（Applicationmanager）通信，ResourceManager检测application的输入输出设置，作业的输入分片是否能够计算等等，如果有任何问题，将会返回错误信息给client</li>
<li>第二步通过之后，便开始上传用户代码jar包和相关文件到HDFS。这一步表明hadoop确实是data-distributed，而不是model-distributed</li>
<li>提交作业</li>
<li>此时applicationmanager会在一个节点上启动一个container（也就是申请的一些资源），然后在上面运行一个applicationmaster。这个applicationmaster会在applicationmanager上进行注册，并且applicationmanager会监管这个applicationmaster，一旦挂了，就重启他</li>
<li>然后applicationmaster开始初始化application，包括创建HDFS目录，生成InputSplit分片信息，获取application的id等等</li>
<li>这一步就是根据具体的inputsplit的要求，用于读取输入数据</li>
<li>然后applicationmaster向resourcemanager以及nodemanager申请更多的计算资源（container），以准备运行application</li>
<li>Nodemanager就根据要求，在这些计算资源（container）上，开个子进程，开始执行MapReduce任务，并汇报任务状态给applicationmaster，一旦有container挂了，就重启。等application执行结束，applicationmaster就向applicationmanager注销，释放资源</li>
</ol>
<blockquote>
<p>Yarn的另一个目标就是拓展Hadoop，使得它不仅仅可以支持MapReduce计算，还能很方便的管理诸如Hive、Hbase、Pig、Spark/Shark等应用。这种新的架构设计能够使得各种类型的应用运行在Hadoop上面，并通过Yarn从系统层面进行统一的管理，也就是说，有了Yarn，各种应用就可以互不干扰的运行在同一个Hadoop系统中，共享整个集群资源，如下图所示
{%asset_img 微信图片_20180725163557.jpg%}</p>
</blockquote>
<p>#####Yarn的优势
Yarn 框架相对于老的 MapReduce 框架什么优势呢？我们可以看到：</p>
<ol>
<li>这个设计大大减小了 JobTracker（也就是现在的 ResourceManager）的资源消耗，并且让监测每一个 Job 子任务 (tasks) 状态的程序分布式化了，更安全、更优美。</li>
<li>在新的 Yarn 中，ApplicationMaster 是一个可变更的部分，用户可以对不同的编程模型写自己的 AppMst，让更多类型的编程模型能够跑在 Hadoop 集群中，可以参考 hadoop Yarn 官方配置模板中的 mapred-site.xml 配置。</li>
<li>对于资源的表示以内存为单位 ( 在目前版本的 Yarn 中，没有考虑 cpu 的占用 )，比之前以剩余 slot 数目更合理。</li>
<li>老的框架中，JobTracker 一个很大的负担就是监控 job 下的 tasks 的运行状况，现在，这个部分就扔给 ApplicationMaster 做了，而 ResourceManager 中有一个模块叫做 ApplicationsMasters( 注意不是 ApplicationMaster)，它是监测 ApplicationMaster 的运行状况，如果出问题，会将其在其他机器上重启。</li>
<li>Container 是 Yarn 为了将来作资源隔离而提出的一个框架。这一点应该借鉴了 Mesos 的工作，目前是一个框架，仅仅提供 java 虚拟机内存的隔离 ,hadoop 团队的设计思路应该后续能支持更多的资源调度和控制 , 既然资源表示成内存量，那就没有了之前的 map slot/reduce slot 分开造成集群资源闲置的尴尬情况。</li>
</ol>
<p>####Spark on Yarn
Spark支持在Yarn上运行，其分为Yarn-cluster模式和Yarn-client模式。</p>
<blockquote>
<p>Spark支持可插拔的集群管理模式(Standalone、Mesos以及YARN)，集群管理负责启动executor进程，编写Spark application的人根本不需要知道Spark用的是什么集群管理。</p>
</blockquote>
<p>Spark支持的三种集群模式，这三种集群模式都由两个组件组成:master和slave。Master服务(<strong>YARN ResourceManager</strong>,Mesos master和Spark standalone master)决定哪些application可以运行，什么时候运行以及哪里去运行。而slave服务( <strong>YARN NodeManager</strong>, Mesos slave和Spark standalone slave)实际上运行executor进程。
当在YARN上运行Spark作业，每个Spark executor作为一个YARN容器(container)运行。Spark可以使得多个Tasks在同一个容器(container)里面运行。这是个很大的优点。</p>
<blockquote>
<p>注意这里和Hadoop的MapReduce作业不一样，MapReduce作业为每个Task开启不同的JVM来运行。虽然说MapReduce可以通过参数来配置。详见mapreduce.job.jvm.numtasks。关于这个参数的介绍已经超过本篇文章的介绍。</p>
</blockquote>
<p>从广义上讲，<strong>yarn-cluster适用于生产环境</strong>；而 <strong>yarn-client适用于交互和调试</strong>，也就是希望快速地看到application的输出。</p>
<blockquote>
<p>在我们介绍yarn-cluster和yarn-client的深层次的区别之前，我们先明白一个概念：Application Master。在YARN中，每个Application实例都有一个Application Master进程，它是Application启动的第一个容器。它负责和ResourceManager打交道，并请求资源。获取资源之后告诉NodeManager为其启动container。</p>
</blockquote>
<p>从深层次的含义讲，yarn-cluster和yarn-client模式的区别其实就是Application Master进程的区别，yarn-cluster模式下，driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行。然而yarn-cluster模式不适合运行交互类型的作业。而yarn-client模式下，Application Master仅仅向YARN请求executor，client会和请求的container通信来调度他们工作，也就是说Client不能离开。看下下面的两幅图应该会明白（上图是yarn-cluster模式，下图是yarn-client模式）：
{%asset_img calxf/Desktop/Hadoop/img/spark-yarn-f31.png%}
{%asset_img spark-yarn-f22.png%}</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[初始Hadoop]]></title>
        <id>https://xiaofine.github.io/post/初始Hadoop</id>
        <link href="https://xiaofine.github.io/post/初始Hadoop">
        </link>
        <updated>2018-03-16T08:07:53.000Z</updated>
        <summary type="html"><![CDATA[<h3 id="hadoop简介">Hadoop简介</h3>
<p>Hadoop是一个于2011年，由Apache基金会所开发的分布式系统基础架构。它为我们提供了一个可靠的，可扩展的分布式计算框架。总的来说，Hadoop由四部分组成，包括:</p>
<ol>
<li>分布式计算框架MapReduce</li>
<li>分布式文件系统HDFS</li>
<li>作业调度和资源管理服务YARN</li>
<li>用于支持外部组件的公用调用服务Hadoop Common</li>
</ol>
<p>本文主要就前三部分进行总结。</p>
]]></summary>
        <content type="html"><![CDATA[<h3 id="hadoop简介">Hadoop简介</h3>
<p>Hadoop是一个于2011年，由Apache基金会所开发的分布式系统基础架构。它为我们提供了一个可靠的，可扩展的分布式计算框架。总的来说，Hadoop由四部分组成，包括:</p>
<ol>
<li>分布式计算框架MapReduce</li>
<li>分布式文件系统HDFS</li>
<li>作业调度和资源管理服务YARN</li>
<li>用于支持外部组件的公用调用服务Hadoop Common</li>
</ol>
<p>本文主要就前三部分进行总结。</p>
<!--more-->
<p>就分布式计算框架而言可以分为 <strong>模型并行(Model Parallelism)</strong> ， <strong>数据并行(Data Parallelism)</strong> 和 <strong>两者的混合(Irregular Parallelism)</strong> 。模型并行化是将模型划分成子模型，分布到各个节点上（比如将线性模型参数W根据feature进行划分）；数据并行是将数据划分到不同节点上，而计算模型本身不会进行划分。
{% asset_img 20170729203732772.png parallelismModel1%}
{% asset_img 20170729203751273.png parallelismModel2%}
根据算法迭代方法的不同，现有的分布式计算框架可以划分为 <strong>基于同步(Synchronization-based)</strong> 的和 <strong>基于异步的(Asynchronization-based)</strong>。两者的不同在于是否等待所有节点完成计算/任务，然后在进入下一次迭代/任务。</p>
<p>对于Hadoop框架来说，Hadoop是一个 <strong>基于同步和数据并行(Synchronization-based and Data-distributed)的计算框架</strong>。Hadoop是建立在HDFS之上，数据本身已经进行了分布式存储与各个节点上，所以Hadoop的MapReduce计算任务会将整个计算模型/程序发送到各个节点上运行。这暗示着Hadoop不可避免的会涉及到数据在不同节点上的调度，同时带来巨大的IO开销。并且也暗示了MapReduce计算框架不能应用于数据之间有依赖的情况。因为HDFS上的数据是基于IID划分的。另一方面，对于每一次MapReduce任务迭代，MapReduce总是等到所有节点都完成了Map操作之后，再进行Reduce操作。当某个节点执行任务特别慢的时候，整个集群会陷入等待状态。考虑到这个问题Hadoop也做了相应的处理。比如当一定比例的节点完成Map任务的时候，就可以进入Reduce阶段，而不在等待；当然也在从任务调度的层面监控作业进度，将某个节点上执行缓慢的任务放在别的节点上执行。</p>
<p>即便Hadoop本身有一些不足，但是也不影响Hadoop的优势：</p>
<ul>
<li>Low cost，搭建Hadoop的成本是很小的，它能利用每一个臭皮匠</li>
<li>Scalability，多个臭皮匠，顶很多诸葛亮</li>
<li>Reliability and Fault Tolerance，Hadoop本身提供很多的容错机制，保证集群的可靠性。包括HDFS层面的冗余备份，HA机制对节点宕机的容错，YARN机制对作业调度的容错支持等等。</li>
<li>Efficiency，分布式计算的优势</li>
<li>Throughput，支持很大的吞吐量，支持一写多读</li>
</ul>
<p>当然，Hadoop也有不能处理的场景</p>
<ol>
<li>小数据与大量小文件。由于HDFS上数据是以块(Block)进行存储的，不足块大小的文件同样会占用一个块的大小。如果小文件太多，会占用NameNode过多的内存来维护元数据信息。</li>
<li>修改文件的值。HDFS不提供Value-level的数据修改，只能覆写或者追加</li>
<li>流式作业和低延时场景。Hadoop MapReduce是操作HDFS上的数据，所以流式数据到来之后，会涉及到在HDFS上的转存操作，然后再是读取数据，交于MapReduce计算。并且MapReduce计算框架本身也会涉及到一些数据溢写的IO操作。因此实时性要求强的场景下，Hadoop并不具有优势</li>
<li>MapReduce多次迭代。Hadoop对于MapReduce的迭代任务的支持并不友好，需要手动构建多个MapReduce Job。对于众多机器学习算法，算法的迭代是难以避免的。这时候Hadoop不是一个很好的选择</li>
<li>基于图的并行计算（Graph parallel computing）。Hadoop是基于数据并行的计算框架，底层HDFS进行数据分布式储存的时候，并没有考虑数据之间的联系，反倒是假设了数据是IID的。因此对于图数据，Hadoop也是不适合的。</li>
</ol>
<p>####Hadoop架构
HDFS是Hadoop Distributed File System的缩写，它采用了主从式的结构。根据节点角色的不同，可以主要分为 <strong>NameNode</strong> 和 <strong>DataNode</strong> （细分还包括Second Namenode，JournalNode等等）。下图是HDFS的粗略框架
{% asset_img 微信图片_20180725142528.jpg HadoopConstruction%}</p>
<p>其中</p>
<ul>
<li><strong>Namenode</strong> 充当班长的角色，管理着整个班级（集群）的信息。这份名单被叫做元数据（meta-data）。另外，客户端的访问请求都由namenode负责。另外namenode上也维护了集群的其他信息，包括命名空间，集群配置信息等等。一般来说，namenode上有两个重要文件常驻内存，提供元数据信息。这两个文件是FsImage和EditLogs。前者记录了命名空间，集群配置信息和数据块的相关信息；后者存放了对文件系统的每一次修改记录。比如新建了一个文件夹，修改了数据块的复制个数等等。Namenode需要周期性的合并两个文件，生成新的FsImage文件，以提供客户端的查询访问。</li>
<li><strong>Datanode</strong> 负责实际数据的存放，数据以数据块（block）的形式存储在datanode之上，每一个数据块都有多个冗余备份（Replication），以达到容错的作用。客户端对数据的读写操作直接作用在datanode上，但前提是客户端首先得由namenode告知在哪里读，在哪里写。</li>
<li>图中的Rack是表示一个PC或者机柜</li>
</ul>
<p>为了保证整个班级的正常次序，班长需要定期的点名，检查是否有童鞋上课睡觉。这里的namenode并不会主动点名，而是让各个datanode主动汇报，定期的给namenode发送信息。信息包括两部分，<strong>心跳检测（HeartBeat）</strong> 和 <strong>数据块汇报信息（BlockReport）</strong>。前者告诉namenode，datanode还活着；后者是该datanode上数据块的信息。</p>
<p>如果HeartBeat和BlockReport被namenode收到了
namenode会汇总大家的blockreport，检测文件的数据块是否有丢失，数据块的复制数是否达到要求。如果没有达到要求，集群会进入 <strong>Safe Mode</strong>。如果namenode超过10min还没有收到某个datanode的HeartBeat
namenode会将这个datanode标记为挂了，然后将原本存储在这个datanode上的数据块，重新复制到其他节点上，并且以后的计算任务也不会再发送给这个datanode了。
当然，班长namenode也可能会挂掉。一旦班长挂了，整个班级都约等于挂了。为了防止这个情况，允许集群的更好的容错能力，Hadoop 1.X启动了副班长策略，引入了 <strong>Second Namenode</strong>。Second namenode负责FsImage和EditLog两个文件的合并，减轻namenode的负担（如下图）。
{%asset_img 微信图片_20180725144501.jpg SecondNN%}</p>
<p>当namenode挂掉了，second namenode上会保存有最新的FsImage文件。那么集群管理员就可以将这份FsImage拷贝到namenode上，然后人工重启namenode。所以second namenode不提供namenode的故障处理，它仅仅是减轻namenode的压力而已。</p>
<p>要达到故障处理的要求，Hadoop 2.X之后提供了 <strong>Hadoop HA策略</strong>。但是，注意，<strong>HA和second namenode策略不能同时使用</strong>。Hadoop HA的框架图如下：
{%asset_img 微信图片_20180725144658.jpg HA%}</p>
<p>Hadoop HA引入了两个Namenode，即 <strong>Active Namename</strong> 和 <strong>Standby Namenode</strong>。但是只有一个提供集群服务，而另一个就standby。一旦active namenode挂了，standby的就立刻上线。至于一开始，哪个namenode充当active的，取决于Zookeeper。Zookeeper提供master选举的作用，这个选举实际上是一个抢占式的锁机制，两个namenode谁先到谁就是active的。</p>
<p>standby namenode需要和active namenode的元信息一致，才能在active one宕掉之后，立刻提供一致服务。为了让元信息一致：</p>
<ol>
<li>集群里的datanode需要同时向两个namenode发送心跳和blockreport；</li>
<li>Active namenode对EditLog的修改需要同时写入JournalNodes（也就是图中的JN）。一般来说，有2N+1台JN来存储EditLog，每次写数据操作有大多数（&gt;=N+1）返回成功时即认为该次写成功。同时，也容忍了最多有N台机器挂掉，如果多于N台挂掉，这个算法就失效了。</li>
<li>任何修改操作在 Active NN上执行时，JN进程同时也会记录修改log到至少半数以上的JN中，这时 Standby NN 监测到JN 里面的同步log发生变化了会读取 JN 里面的修改log，然后同步到自己的的目录镜像树里面</li>
</ol>
<p>最后，图中的FailoverController监控了NameNode的状态，并且通过心跳机制，监控自己在zookeeper中的状态。一旦active namenode挂了，则触发standby的namenode上线。</p>
<p>Hadoop HA机制虽然提供了故障处理，但是它任然限制了只能有一个namenode提供服务。并且，如果hdfs中有很多块，那么元数据将占用namenode大量的内存。为了处理这个问题，Hadoop 2.X提供了HDFS Federation机制（NN联邦），它允许多个namenode共用一个集群里的存储资源，每个namenode都可以单独对外提供服务。</p>
<p>####Hadoop读写
在HDFS中，数据是以数据块（Block）的形式存储，不同版本Hadoop的默认块大小不同，128M或者64M。这个值可以用户自己定义。如果一个数据文件被划分成越小的数据块，HDFS读取这个文件时候的并发性也就更高。但是，也意味着将带来更多的磁盘寻道的IO开销，所以这是个trade-off。
HDFS采用了冗余备份的策略，为每一个数据块都保存了多个复制（replicas）。默认replicas的大小为3份。下面展示了HDFS上数据的存储图：
{% asset_img 微信图片_20180725145432.jpg Split%}</p>
<ul>
<li>图中可以再次看到namenode上存储了文件到数据块的映射信息，比如文件part-0，有两个数据块1,3。每个数据块一共有两个复制，分别存储在不同的节点上。事实上，数据块的总数之间影响了MapReduce过程中，mapper的个数。</li>
<li>大文件被划分成了很多的block，而不足block大小的文件（比如一个只有1k的文件），同样占用了一个block。</li>
<li>HDFS对文件的划分存储并没有考虑到文件的结构信息。这时候，HDFS引入了InputSplit，InputSplit由用户在读取HDFS数据的时候指定，它保证了一个文件的逻辑划分。比如，下图是一个由4条记录组成的文件，每条记录100M。
{%asset_img 微信图片_20180725152143.jpg SplitExample1%}
这时候，启动一个mapper来解析任何一个Block都不能都到正确的结果，比较record的结构以及被破坏。为了处理这里问题，我们需要指定一个InputSplit来读取数据，给出每一条record在逻辑上的划分。PS：普通文本文件当然可以不用设置InputSplit。
{%asset_img 微信图片_20180725152247.jpg SplitExmaple2%}
具体来说，需要在设置Job的时候，调用job.setInputFormatClass(WholeFileInputFormat.class)来指定自己实现的InputSplit格式。这里的WholeFileInputFormat.class是自己实现的类，用来指定split，它需要继承FileInputFormat类（当然选择TextInputFormat，SequenceFileInputFormat等等类来集成也是可以的，具体场景，具体分析），覆写createRecordReader和isSplitable方法。同时还要指定自己的RecordReader。下面是一个fengfeng写的小栗子，用于解析一个二进制文件。</li>
</ul>
<pre><code>public class WholeFileInputFormat extends FileInputFormat&lt;Text, BytesWritable&gt; {
    @Override
    public RecordReader&lt;Text, BytesWritable&gt; createRecordReader(
            InputSplit split, TaskAttemptContext context) throws IOException,
            InterruptedException {
      //这个类告诉程序要怎么去读文件，找出正确的input划分
        WholeFileRecordReader recordReader = new WholeFileRecordReader();  
        recordReader.initialize(split,context);
        return recordReader;
    }
    @Override
    protected boolean isSplitable(JobContext context, Path filename) {
        return false;
    }
}
 
public class WholeFileRecordReader extends RecordReader&lt;Text, BytesWritable&gt; {
    private FileSplit fileSplit;
    private FSDataInputStream fin;
    private Text key = new Text();
    private BytesWritable value = new BytesWritable();
    private boolean processed = false;
    private Configuration conf;
    private String fileName;
    private int count=0;
    @Override
    public void initialize(InputSplit inputSplit, TaskAttemptContext context)
            throws IOException, InterruptedException {
        //这里整一些初始化的工作
    }
    @Override
    public boolean nextKeyValue() {
        // 这个方法里需要实现具体这么解析一条一条的记录，然后将读取结果设置到key和value里
        // 期间，有任何解析问题就返回false，否则返回true
        value = new BytesWritable(info);
         key.set(count+&quot;&quot;);
         return true;
    }
    
    @Override
    public float getProgress() throws IOException, InterruptedException {
        // TODO Auto-generated method stub
        return processed? fileSplit.getLength():0;
    }
    @Override
    public void close() throws IOException {
        // TODO Auto-generated method stub
        //fis.close();
    }
    @Override
    public Text getCurrentKey() throws IOException, InterruptedException {
        // TODO Auto-generated method stub
        return this.key;
    }
    @Override
    public BytesWritable getCurrentValue() throws IOException,
            InterruptedException {
        // TODO Auto-generated method stub
        return this.value;
    }
}
</code></pre>
<p>在了解了数据是怎么存储了之后，我们再来了解下客户端是如何读写数据的。
#####读数据
由于namenode上存储了datanode和数据块的路径地址，所以，客户端实际读取数据之前，需要访问namenode获取相关信息。而namenode会计算最佳读取的块，然后返回其位置信息给客户端。最后客户端需要根据返回的信息，自己去找datanode读取数据。流程如下图所示。
{%asset_img 微信图片_20180725155109.jpg %}
幸运的是，客户端具体的程序调用，已经屏蔽了读取数据的细节：</p>
<pre><code>Configuration configuration = new Configuration();
String dataPath = &quot;hdfs://localhost:9000/dml&quot;;
FileSystem fileSystem = FileSystem.get(URI.create(dataPath), configuration);
FSDataInputStream fsDataInputStream = fileSystem.open(new Path(dataPath));
BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(fsDataInputStream));
String line = bufferedReader.readLine();
while(line != null){
    System.out.println(line);
    line = bufferedReader.readLine();
}
bufferedReader.close();
fsDataInputStream.close();
fileSystem.close();
</code></pre>
<ol>
<li>客户端(client)用FileSystem的open()函数打开文件</li>
<li>DistributedFileSystem用RPC调用namenode，得到文件的数据块信息。对于每一个数据块，namenode节点返回保存数据块的数据节点的地址。DistributedFileSystem返回FSDataInputStream给客户端，用来读取数据。</li>
<li>客户端调用stream的read()函数开始读取数据。</li>
<li><strong>DFSInputStream</strong> 连接保存此文件第一个数据块的最近的数据节点。Data从数据节点读到客户端(client).
当此数据块读取完毕时，DFSInputStream关闭和此数据节点的连接，然后连接此文件下一个数据块的最近的数据节点。</li>
<li>当客户端读取完毕数据的时候，调用FSDataInputStream的close函数。在读取数据的过程中，如果客户端在与数据节点通信出现错误，则尝试连接包含此数据块的下一个数据节点。失败的数据节点将被记录，以后不再连接。</li>
</ol>
<p>#####写数据
客户端写数据的流程如下所示：
<img src="/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20180725160449.jpg" alt="ReadData">
同样，程序实现：</p>
<pre><code>Configuration configuration = new Configuration();
String dst = &quot;hdfs://localhost:9000/dml&quot;;
String data = &quot;data mining lab data mining lab,data mining lab&quot;;
FileSystem fileSystem = FileSystem.get(URI.create(dst), configuration);
Path path = new Path(dst);
FSDataOutputStream fsDataOutputStream = fileSystem.create(path);
fsDataOutputStream.write(data.getBytes());
fsDataOutputStream.flush();
fsDataOutputStream.close();
</code></pre>
<ol>
<li>客户端调用create()来创建文件</li>
<li>DistributedFileSystem用RPC调用元数据节点，在文件系统的命名空间中创建一个新的文件。元数据节点首先确定文件原来不存在，并且客户端有创建文件的权限，然后创建新文件。DistributedFileSystem返回DFSOutputStream，客户端用于写数据。</li>
</ol>
<blockquote>
<p>RPC(Remote Procedure Call)：一种通信协议，该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用</p>
</blockquote>
<ol start="3">
<li>客户端开始写入数据，DFSOutputStream将数据分成块，写入data queue。</li>
<li>Data queue由Data Streamer读取，并通知元数据节点分配数据节点，用来存储数据块(每块默认复制3块)。分配的数据节点放在一个pipeline里。</li>
<li>Data Streamer将数据块写入pipeline中的第一个数据节点。第一个数据节点将数据块发送给第二个数据节点。第二个数据节点将数据发送给第三个数据节点。DFSOutputStream为发出去的数据块保存了ack queue，等待pipeline中的数据节点告知数据已经写入成功。如果数据节点在写入的过程中失败：关闭pipeline，将ack queue中的数据块放入data queue的开始。当前的数据块在已经写入的数据节点中被元数据节点赋予新的标示，则错误节点重启后能够察觉其数据块是过时的，然后会被删除。失败的数据节点从pipeline中移除，另外的数据块，则写入pipeline中的另外两个数据节点。元数据节点则被通知此数据块是复制块数不足，将来会再创建第三份备份。</li>
<li>当客户端结束写入数据，则调用stream的close函数。</li>
<li>此操作将所有的数据块写入pipeline中的数据节点，并等待ack queue返回成功。最后通知元数据节点写入完毕。</li>
</ol>
]]></content>
    </entry>
</feed>